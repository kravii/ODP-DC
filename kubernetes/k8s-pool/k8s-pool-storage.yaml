---
# K8s Pool Storage Configuration
# Dedicated storage for Kubernetes resource pool (isolated from VM pool)

apiVersion: v1
kind: Namespace
metadata:
  name: k8s-pool-storage
  labels:
    name: k8s-pool-storage
    purpose: k8s-storage-management
    pool: k8s
    isolation: enabled
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-pool-storage-config
  namespace: k8s-pool-storage
data:
  storage-info: |
    # K8s Pool Storage Configuration
    # Dedicated 1.8TB storage per server for Kubernetes only
    # Isolated from VM pool - no cross-pool access
    
    # Storage Allocation per K8s Server (1.8TB):
    # - Databases: 500GB
    # - Applications: 1000GB
    # - Monitoring: 200GB
    # - Logs: 50GB
    # - Backups: 50GB
    
    # Directory Structure:
    # /k8s-storage/
    # ├── databases/        # Database storage
    # ├── applications/     # Application data
    # ├── monitoring/       # Monitoring data
    # ├── logs/            # Log storage
    # └── backups/         # Backup storage
  pool-isolation: |
    # Resource Pool Isolation Rules
    # K8s pool servers: 10.0.1.0/24
    # VM pool servers: 10.0.2.0/24
    # Cross-pool access: DENIED
---
apiVersion: batch/v1
kind: Job
metadata:
  name: setup-k8s-pool-storage
  namespace: k8s-pool-storage
spec:
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        kubernetes.io/hostname: "k8s-pool-node"  # Only run on K8s pool nodes
      containers:
      - name: setup-storage
        image: alpine:latest
        command:
        - /bin/sh
        - -c
        - |
          # Create K8s pool storage directories
          mkdir -p /k8s-storage/{databases,applications,monitoring,logs,backups}
          
          # Set permissions for K8s pool
          chmod -R 755 /k8s-storage
          chown -R 1000:1000 /k8s-storage
          
          # Create storage configuration
          cat > /k8s-storage/storage-config.yaml << 'EOF'
          k8s_pool_storage:
            pool_name: "k8s"
            isolation_enabled: true
            total_storage_gb: 1800
            allocations:
              databases: 500
              applications: 1000
              monitoring: 200
              logs: 50
              backups: 50
            deny_vm_pool_access: true
          EOF
          
          # Set up storage monitoring
          cat > /k8s-storage/monitor-k8s-storage.sh << 'EOF'
          #!/bin/sh
          echo "=== K8s Pool Storage Usage ==="
          df -h /k8s-storage
          echo ""
          echo "=== K8s Pool Directory Usage ==="
          du -sh /k8s-storage/*
          echo ""
          echo "=== K8s Pool Storage Health ==="
          # Check if VM pool access is blocked
          if ping -c 1 -W 1 10.0.2.1 >/dev/null 2>&1; then
            echo "WARNING: VM pool network accessible - isolation may be compromised"
          else
            echo "OK: VM pool network isolated"
          fi
          EOF
          
          chmod +x /k8s-storage/monitor-k8s-storage.sh
          
          echo "K8s pool storage setup completed"
        volumeMounts:
        - name: k8s-storage
          mountPath: /k8s-storage
      volumes:
      - name: k8s-storage
        hostPath:
          path: /
---
# K8s Pool Local Path Provisioner
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-pool-local-path-provisioner-service-account
  namespace: k8s-pool-storage
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8s-pool-local-path-provisioner-role
rules:
- apiGroups: [""]
  resources: ["nodes", "persistentvolumes"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch", "create", "delete", "patch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["create", "update", "patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-pool-local-path-provisioner-bind
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8s-pool-local-path-provisioner-role
subjects:
- kind: ServiceAccount
  name: k8s-pool-local-path-provisioner-service-account
  namespace: k8s-pool-storage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-pool-local-path-provisioner
  namespace: k8s-pool-storage
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-pool-local-path-provisioner
  template:
    metadata:
      labels:
        app: k8s-pool-local-path-provisioner
    spec:
      serviceAccountName: k8s-pool-local-path-provisioner-service-account
      nodeSelector:
        pool: k8s  # Only run on K8s pool nodes
      containers:
      - name: k8s-pool-local-path-provisioner
        image: rancher/local-path-provisioner:v0.0.24
        imagePullPolicy: IfNotPresent
        command:
        - local-path-provisioner
        - start
        - --config
        - /etc/config/config.json
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config/
        - name: storage-path
          mountPath: /opt/local-path-provisioner
        - name: k8s-storage
          mountPath: /k8s-storage
      volumes:
      - name: config-volume
        configMap:
          name: k8s-pool-local-path-config
      - name: storage-path
        hostPath:
          path: /opt/local-path-provisioner
      - name: k8s-storage
        hostPath:
          path: /
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-pool-local-path-config
  namespace: k8s-pool-storage
data:
  config.json: |
    {
      "nodePathMap":[
      {
        "node": "DEFAULT_PATH_FOR_NON_LISTED_NODES",
        "paths": ["/k8s-storage"]
      }
      ]
    }
---
# K8s Pool Storage Classes
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-database-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: rancher.io/local-path
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: pool
    values: ["k8s"]
parameters:
  path: "/k8s-storage/databases"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-app-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: pool
    values: ["k8s"]
parameters:
  path: "/k8s-storage/applications"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-monitoring-storage
provisioner: rancher.io/local-path
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: pool
    values: ["k8s"]
parameters:
  path: "/k8s-storage/monitoring"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-log-storage
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: pool
    values: ["k8s"]
parameters:
  path: "/k8s-storage/logs"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: k8s-backup-storage
provisioner: rancher.io/local-path
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: pool
    values: ["k8s"]
parameters:
  path: "/k8s-storage/backups"
---
# K8s Pool Storage Monitor
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: k8s-pool-storage-monitor
  namespace: k8s-pool-storage
spec:
  selector:
    matchLabels:
      app: k8s-pool-storage-monitor
  template:
    metadata:
      labels:
        app: k8s-pool-storage-monitor
    spec:
      nodeSelector:
        pool: k8s  # Only run on K8s pool nodes
      containers:
      - name: k8s-pool-storage-monitor
        image: alpine:latest
        command:
        - /bin/sh
        - -c
        - |
          while true; do
            echo "=== K8s Pool Storage Report $(date) ==="
            
            # Check K8s storage usage
            echo "K8s Pool Storage Usage:"
            df -h /k8s-storage
            echo ""
            
            # Check directory usage
            echo "K8s Pool Directory Usage:"
            du -sh /k8s-storage/*
            echo ""
            
            # Check isolation
            echo "K8s Pool Isolation Check:"
            if ping -c 1 -W 1 10.0.2.1 >/dev/null 2>&1; then
              echo "WARNING: VM pool network accessible - isolation compromised"
            else
              echo "OK: VM pool network isolated"
            fi
            
            # Check VM pool storage access
            if [ -d "/vm-storage" ]; then
              echo "ERROR: VM pool storage accessible - isolation compromised"
            else
              echo "OK: VM pool storage isolated"
            fi
            
            echo "K8s pool storage monitoring completed"
            echo "======================================"
            sleep 300  # Check every 5 minutes
          done
        volumeMounts:
        - name: k8s-storage
          mountPath: /k8s-storage
      volumes:
      - name: k8s-storage
        hostPath:
          path: /
---
# K8s Pool Storage Cleanup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: k8s-pool-storage-cleanup
  namespace: k8s-pool-storage
spec:
  schedule: "0 2 * * *"  # Run daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          nodeSelector:
            pool: k8s  # Only run on K8s pool nodes
          containers:
          - name: cleanup
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              echo "Starting K8s pool storage cleanup..."
              
              # Clean up old backup files (older than 30 days)
              find /k8s-storage/backups -type f -mtime +30 -delete 2>/dev/null || true
              echo "Cleaned up old backup files"
              
              # Clean up old log files (older than 7 days)
              find /k8s-storage/logs -type f -mtime +7 -delete 2>/dev/null || true
              echo "Cleaned up old log files"
              
              # Clean up temporary files
              find /k8s-storage -name "*.tmp" -delete 2>/dev/null || true
              find /k8s-storage -name "*.temp" -delete 2>/dev/null || true
              echo "Cleaned up temporary files"
              
              # Verify isolation
              if [ -d "/vm-storage" ]; then
                echo "ERROR: VM pool storage accessible during cleanup"
                exit 1
              fi
              
              echo "K8s pool storage cleanup completed"
            volumeMounts:
            - name: k8s-storage
              mountPath: /k8s-storage
          volumes:
          - name: k8s-storage
            hostPath:
              path: /
          restartPolicy: OnFailure